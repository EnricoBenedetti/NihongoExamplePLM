{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this class will load index etc, and provide results...\n",
    "import utils\n",
    "import torch\n",
    "from inverted_index import InvertedIndex\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import \\\n",
    "    AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline, pipeline\n",
    "from retrieval import Retrieval\n",
    "import retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'retrieval' from '/disks/disk00/enrico_benedetti/nihongoexample/code/scripts/retrieval.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(utils)\n",
    "importlib.reload(retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset' : \"bennexx/jp_sentences\",\n",
    "          'spacy_model': 'ja_ginza',\n",
    "          'index_file': '/data/enrico_benedetti/nihongoexample/data/corpus_all/jp_sentences/inverted_index.pkl',\n",
    "          'sense_model': 'bennexx/mirrorwic-cl-tohoku-bert-base-japanese-v3',\n",
    "          #'diff_model': \"/data/enrico_benedetti/nihongoexample/code/models/checkpoint-66\"\n",
    "          'diff_model': 'bennexx/cl-tohoku-bert-base-japanese-v3-jlpt-classifier',\n",
    "          # 'device': 'cpu',\n",
    "          'device': 0,\n",
    "          'save_dir' : '../../evaluation/outputs/experiments',\n",
    "          #'generation': True\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = retr.index\n",
    "# nlp = retr.nlp\n",
    "# df = retr.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'ja_ginza' (5.1.3) was trained with spaCy v3.2.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: /disks/disk00/enrico_benedetti/nihongoexample/data/corpus_all/jp_sentences/inverted_index.pkl\n"
     ]
    }
   ],
   "source": [
    "retr = Retrieval(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l, _ = retr.get_sentence_list('日本語能力試験', '日本語能力試験は簡単です。', target_level='N5')\n",
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>level</th>\n",
       "      <th>level_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>日本語能力試験は簡単です。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本語能力試験は簡単です。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本語能力試験は簡単です。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>日本語能力試験は簡単です。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>日本語能力試験は簡単です。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentence level  level_score\n",
       "0  日本語能力試験は簡単です。    N5          1.0\n",
       "1  日本語能力試験は簡単です。    N5          1.0\n",
       "2  日本語能力試験は簡単です。    N5          1.0\n",
       "3  日本語能力試験は簡単です。    N5          1.0\n",
       "4  日本語能力試験は簡単です。    N5          1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/transformers/pipelines/base.py:1043: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>level</th>\n",
       "      <th>level_score</th>\n",
       "      <th>sense_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>sentence_docs</th>\n",
       "      <th>parse_tree</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>syntax_div_score</th>\n",
       "      <th>lexical_div_score</th>\n",
       "      <th>div_score</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>自分の意思で、内部に入ろうと思わなければ、中には入れない。</td>\n",
       "      <td>N3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(自分, の, 意思, で, 、, 内部, に, 入ろう, と, 思わ, なけれ, ば, 、...</td>\n",
       "      <td>[[[(NOUN_advcl (NOUN_nmod ADP_case) AUX_cop PU...</td>\n",
       "      <td>[自分, の, 意思, で, 、, 内部, に, 入ろう, と, 思わ, なけれ, ば, 、...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>私は部屋に入る前に、靴を脱ぎます。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.709723</td>\n",
       "      <td>0.854861</td>\n",
       "      <td>(私, は, 部屋, に, 入る, 前, に, 、, 靴, を, 脱ぎ, ます, 。)</td>\n",
       "      <td>[[ADP_case], [[(NOUN_obl ADP_case)], ADP_case,...</td>\n",
       "      <td>[私, は, 部屋, に, 入る, 前, に, 、, 靴, を, 脱ぎ, ます, 。]</td>\n",
       "      <td>0.393987</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>0.669650</td>\n",
       "      <td>0.762256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>犬が庭に入ると、鳥が飛び立ちます。</td>\n",
       "      <td>N4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.663189</td>\n",
       "      <td>0.631595</td>\n",
       "      <td>(犬, が, 庭, に, 入る, と, 、, 鳥, が, 飛び立ち, ます, 。)</td>\n",
       "      <td>[[[ADP_case], [ADP_case], SCONJ_mark, PUNCT_pu...</td>\n",
       "      <td>[犬, が, 庭, に, 入る, と, 、, 鳥, が, 飛び立ち, ます, 。]</td>\n",
       "      <td>0.440480</td>\n",
       "      <td>0.908259</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.652982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>自分の家に入ると、いつもリラックスできます。</td>\n",
       "      <td>N4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.629022</td>\n",
       "      <td>0.614511</td>\n",
       "      <td>(自分, の, 家, に, 入る, と, 、, いつも, リラックス, でき, ます, 。)</td>\n",
       "      <td>[[[(NOUN_nmod ADP_case), ADP_case], SCONJ_mark...</td>\n",
       "      <td>[自分, の, 家, に, 入る, と, 、, いつも, リラックス, でき, ます, 。]</td>\n",
       "      <td>0.424218</td>\n",
       "      <td>0.852033</td>\n",
       "      <td>0.638126</td>\n",
       "      <td>0.626318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>学校に入る前に、朝ごはんを食べるのが大切です。</td>\n",
       "      <td>N4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.519222</td>\n",
       "      <td>0.559611</td>\n",
       "      <td>(学校, に, 入る, 前, に, 、, 朝ごはん, を, 食べる, の, が, 大切, で...</td>\n",
       "      <td>[[[(VERB_acl (NOUN_obl ADP_case)), ADP_case, P...</td>\n",
       "      <td>[学校, に, 入る, 前, に, 、, 朝ごはん, を, 食べる, の, が, 大切, で...</td>\n",
       "      <td>0.429024</td>\n",
       "      <td>0.812508</td>\n",
       "      <td>0.620766</td>\n",
       "      <td>0.590189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>バスに乗る前に、切符を買ってください。</td>\n",
       "      <td>N5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(バス, に, 乗る, 前, に, 、, 切符, を, 買っ, て, ください, 。)</td>\n",
       "      <td>[[[(NOUN_obl ADP_case)], ADP_case, PUNCT_punct...</td>\n",
       "      <td>[バス, に, 乗る, 前, に, 、, 切符, を, 買っ, て, ください, 。]</td>\n",
       "      <td>0.475755</td>\n",
       "      <td>0.811285</td>\n",
       "      <td>0.643520</td>\n",
       "      <td>0.321760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        sentence level  level_score  sense_score  \\\n",
       "0  自分の意思で、内部に入ろうと思わなければ、中には入れない。    N3          0.2     0.000000   \n",
       "1              私は部屋に入る前に、靴を脱ぎます。    N5          1.0     0.709723   \n",
       "2              犬が庭に入ると、鳥が飛び立ちます。    N4          0.6     0.663189   \n",
       "3         自分の家に入ると、いつもリラックスできます。    N4          0.6     0.629022   \n",
       "4        学校に入る前に、朝ごはんを食べるのが大切です。    N4          0.6     0.519222   \n",
       "5            バスに乗る前に、切符を買ってください。    N5          1.0    -1.000000   \n",
       "\n",
       "   quality_score                                      sentence_docs  \\\n",
       "0       0.100000  (自分, の, 意思, で, 、, 内部, に, 入ろう, と, 思わ, なけれ, ば, 、...   \n",
       "1       0.854861        (私, は, 部屋, に, 入る, 前, に, 、, 靴, を, 脱ぎ, ます, 。)   \n",
       "2       0.631595          (犬, が, 庭, に, 入る, と, 、, 鳥, が, 飛び立ち, ます, 。)   \n",
       "3       0.614511     (自分, の, 家, に, 入る, と, 、, いつも, リラックス, でき, ます, 。)   \n",
       "4       0.559611  (学校, に, 入る, 前, に, 、, 朝ごはん, を, 食べる, の, が, 大切, で...   \n",
       "5       0.000000        (バス, に, 乗る, 前, に, 、, 切符, を, 買っ, て, ください, 。)   \n",
       "\n",
       "                                          parse_tree  \\\n",
       "0  [[[(NOUN_advcl (NOUN_nmod ADP_case) AUX_cop PU...   \n",
       "1  [[ADP_case], [[(NOUN_obl ADP_case)], ADP_case,...   \n",
       "2  [[[ADP_case], [ADP_case], SCONJ_mark, PUNCT_pu...   \n",
       "3  [[[(NOUN_nmod ADP_case), ADP_case], SCONJ_mark...   \n",
       "4  [[[(VERB_acl (NOUN_obl ADP_case)), ADP_case, P...   \n",
       "5  [[[(NOUN_obl ADP_case)], ADP_case, PUNCT_punct...   \n",
       "\n",
       "                                           tokenized  syntax_div_score  \\\n",
       "0  [自分, の, 意思, で, 、, 内部, に, 入ろう, と, 思わ, なけれ, ば, 、...               NaN   \n",
       "1        [私, は, 部屋, に, 入る, 前, に, 、, 靴, を, 脱ぎ, ます, 。]          0.393987   \n",
       "2          [犬, が, 庭, に, 入る, と, 、, 鳥, が, 飛び立ち, ます, 。]          0.440480   \n",
       "3     [自分, の, 家, に, 入る, と, 、, いつも, リラックス, でき, ます, 。]          0.424218   \n",
       "4  [学校, に, 入る, 前, に, 、, 朝ごはん, を, 食べる, の, が, 大切, で...          0.429024   \n",
       "5        [バス, に, 乗る, 前, に, 、, 切符, を, 買っ, て, ください, 。]          0.475755   \n",
       "\n",
       "   lexical_div_score  div_score  total_score  \n",
       "0                NaN        NaN          NaN  \n",
       "1           0.945312   0.669650     0.762256  \n",
       "2           0.908259   0.674370     0.652982  \n",
       "3           0.852033   0.638126     0.626318  \n",
       "4           0.812508   0.620766     0.590189  \n",
       "5           0.811285   0.643520     0.321760  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generation test\n",
    "\n",
    "candidates = ['自分の意思で、内部に入ろうと思わなければ、中には入れない。','私は部屋に入る前に、靴を脱ぎます。', '犬が庭に入ると、鳥が飛び立ちます。', '学校に入る前に、朝ごはんを食べるのが大切です。', 'バスに乗る前に、切符を買ってください。', '自分の家に入ると、いつもリラックスできます。']\n",
    "#candidates = ['日本語能力試験は簡単です。']*6\n",
    "candidates = pd.DataFrame({'sentence': candidates})\n",
    "candidates\n",
    "final_list, _ = retr.get_sentence_list('入る', '自分の意思で、内部に入ろうと思わなければ、中には入れない。', candidates=candidates, target_level='N5')\n",
    "final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list, short_list = retr.get_sentence_list('日本語能力試験', '日本語能力試験は簡単です。', target_level='N5')\n",
    "#sentence_list, short_list = retr.get_sentence_list('良い', '味噌を食事の一部にすることは非常に良いと考えられている。', target_level='N5')\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df = pd.read_csv('../../data/targets/target_words.csv')\n",
    "# levels = ['N1', 'N2', 'N3', 'N4', 'N5']\n",
    "# for level in levels:\n",
    "#     for id, row in tqdm(target_df.iterrows()):\n",
    "#         print(f'level {level}, searching for {row[\"target_word\"]}...')\n",
    "#         target_word = row['target_word']\n",
    "#         context_sentence = row['context_sentence']\n",
    "#         list_ids, _, _ = retr.get_candidates(target_word, context_sentence)\n",
    "#         target_df.loc[id, 'number_hits_in_corpus_cap'] = len(list_ids) - 1 # because the context sentence is here\n",
    "#         list_ids, _, _ = retr.get_candidates(target_word, context_sentence, max_hits=100000000)\n",
    "#         target_df.loc[id, 'number_hits_in_corpus_tot'] = len(list_ids) - 1 # because the context sentence is here\n",
    "\n",
    "# target_df['number_hits_in_corpus_cap'] = target_df['number_hits_in_corpus_cap'].astype(int)\n",
    "# target_df['number_hits_in_corpus_tot'] = target_df['number_hits_in_corpus_tot'].astype(int)\n",
    "# # target_df.drop('number_hits_in_corpus', axis=1, inplace=True)\n",
    "# target_df.to_csv('../../data/targets/target_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "# read the file of target words\n",
    "# search for every level, every word + context\n",
    "\n",
    "# target_df = pd.read_csv('../../data/targets/target_words.csv')\n",
    "# levels = ['N1', 'N2', 'N3', 'N4', 'N5']\n",
    "# for level in levels:\n",
    "#     for id, row in tqdm(target_df.iterrows()):\n",
    "#         print(f'level {level}, searching for {row[\"target_word\"]}...')\n",
    "#         target_word = row['target_word']\n",
    "#         context_sentence = row['context_sentence']\n",
    "#         try:\n",
    "#             retr.get_sentence_list(target_word, context_sentence, k=5, target_level=level)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "target_df = pd.read_csv('../../data/targets/target_words.csv')\n",
    "# Define the directory where your CSV files are located\n",
    "directory = '/data/enrico_benedetti/nihongoexample/evaluation/outputs/retrieval/'\n",
    "\n",
    "# Create a dictionary to store the counts for each file\n",
    "file_counts = {}\n",
    "\n",
    "# List all files in the directory\n",
    "file_list = os.listdir(directory)\n",
    "\n",
    "# Iterate through the list of files\n",
    "for filename in file_list:\n",
    "    # Split the filename into parts using '_' as a delimiter\n",
    "    parts = filename.split('_')\n",
    "    \n",
    "    # Extract the file name without the version information\n",
    "    if len(parts) < 3:\n",
    "        continue\n",
    "    base_filename = parts[-3]\n",
    "    #print(base_filename)\n",
    "    \n",
    "    # If the base filename is already in the dictionary, increment the count\n",
    "    if base_filename in file_counts:\n",
    "        file_counts[base_filename] += 1\n",
    "    else:\n",
    "        # If it's not in the dictionary, add it with a count of 1\n",
    "        file_counts[base_filename] = 1\n",
    "\n",
    "missing = []\n",
    "# Iterate through the dictionary and print files with less than 5 versions\n",
    "for filename, count in file_counts.items():\n",
    "    if count < 5:\n",
    "        print(f\"File '{filename}' has {count} versions (less than 5).\")\n",
    "        missing.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix manually the missing versions\n",
    "# target_word = 'やる'\n",
    "# mask = target_df['target_word'] == target_word\n",
    "# context = target_df.loc[mask,'context_sentence'].values[0]\n",
    "# context\n",
    "# sentence_list, short_list = retr.get_sentence_list(target_word, context, target_level='N4')\n",
    "# sentence_list, short_list = retr.get_sentence_list(target_word, context, target_level='N5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
