{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset prep before implementing a very basic difficulty level estimator for sentences.\n",
    "\n",
    "after removing duplicates (~100) we have\n",
    "jlpt\n",
    "N3    1457\n",
    "N2    1042\n",
    "N4     956\n",
    "N1     779\n",
    "N5     414\n",
    "\n",
    "splits are 60-20-20 uniformly sampled (label not considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>図書館で宿題をしたあとで、帰ります。</td>\n",
       "      <td>N5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今から少し練習したあとで、前で発表してもらいます。</td>\n",
       "      <td>N5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>電車に乗ったあとで、学校に忘れ物をしたことに気がつきました。</td>\n",
       "      <td>N5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>アニメを見たあとで、お風呂に入ります。</td>\n",
       "      <td>N5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>スマホで写真を撮ったあとで、少し色を変えたり、サイズを変えたりします。</td>\n",
       "      <td>N5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>いくらかわいくても、毎⽇孫のめんどうを⾒るのは、正直つらいものがある。</td>\n",
       "      <td>N2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>残念なことに、いちばん会いたかった佐藤さんが来られなくなった。</td>\n",
       "      <td>N2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>⺟は⼀⼈で四⼈の⼦どもを育てたどれほど⼤変だったことか。</td>\n",
       "      <td>N2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>もうすこし野菜の値段が下がらないものか。</td>\n",
       "      <td>N2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5183</th>\n",
       "      <td>いつかは広い家に住んで、⽝を飼いたいものだ。</td>\n",
       "      <td>N2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5184 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  sentence level\n",
       "0                       図書館で宿題をしたあとで、帰ります。    N5\n",
       "1                今から少し練習したあとで、前で発表してもらいます。    N5\n",
       "2           電車に乗ったあとで、学校に忘れ物をしたことに気がつきました。    N5\n",
       "3                      アニメを見たあとで、お風呂に入ります。    N5\n",
       "4      スマホで写真を撮ったあとで、少し色を変えたり、サイズを変えたりします。    N5\n",
       "...                                    ...   ...\n",
       "5179   いくらかわいくても、毎⽇孫のめんどうを⾒るのは、正直つらいものがある。    N2\n",
       "5180       残念なことに、いちばん会いたかった佐藤さんが来られなくなった。    N2\n",
       "5181          ⺟は⼀⼈で四⼈の⼦どもを育てたどれほど⼤変だったことか。    N2\n",
       "5182                  もうすこし野菜の値段が下がらないものか。    N2\n",
       "5183                いつかは広い家に住んで、⽝を飼いたいものだ。    N2\n",
       "\n",
       "[5184 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/loading\n",
    "\n",
    "data_dir = Path('/data/enrico_benedetti/nihongoexample/data/Webscraping/splits/')\n",
    "data_split_dir = Path('/data/enrico_benedetti/nihongoexample/data/Webscraping/splits/')\n",
    "# relative to data dir\n",
    "data_file = Path('train.csv')\n",
    "df = pd.read_csv(data_dir.joinpath(data_file), usecols=['sentence', 'level'])[['sentence', 'level']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "N3    1457\n",
       "N1    1185\n",
       "N2    1172\n",
       "N4     956\n",
       "N5     414\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset='sentence')\n",
    "df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split into train, val, test based on proportion in dataset\n",
    "# # if using cross validation then..?\n",
    "# df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['jlpt'], random_state=42)\n",
    "# df_train, df_val = train_test_split(df_train, test_size=0.25, stratify=df_train['jlpt'], random_state=42)\n",
    "# print(df_train['jlpt'].value_counts(), df_val['jlpt'].value_counts(), df_test['jlpt'].value_counts())\n",
    "# print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE EVERYTHING AS TRAIN\n",
    "df.to_csv(data_split_dir.joinpath('train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, val, test NOT based on proportion in dataset > kind of similar but we keep the simpler one\n",
    "# if using cross validation then..?\n",
    "#df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train, df_val = train_test_split(df, train_size=0.95, random_state=42)\n",
    "#print(df_train['jlpt'].value_counts(), df_val['jlpt'].value_counts(), df_test['jlpt'].value_counts())\n",
    "#print(df_train.shape, df_val.shape, df_test.shape)\n",
    "\n",
    "# reset index\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "#df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# write them to file for ease - index = false does not print the line number\n",
    "df_train.to_csv(\"/data/enrico_benedetti/nihongoexample/data/Webscraping/splits_val/train.csv\")\n",
    "df_val.to_csv(\"/data/enrico_benedetti/nihongoexample/data/Webscraping/splits_val/val.csv\")\n",
    "#df_test.to_csv(data_split_dir.joinpath('old/test.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
