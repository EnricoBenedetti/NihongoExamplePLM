{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import fkassim.FastKassim as fkassim\n",
    "import ginza\n",
    "from ginza import lemma_, bunsetu, sub_phrases\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "# fkassim.download() only run first time, or never since its for japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunsetsu_sub(token):\n",
    "    \"\"\"Performs substitution similar to Tolmachev et al.\"\"\"\n",
    "    result = []\n",
    "    # bunsetsu_head_list = list(ginza.bunsetu_head_tokens(token.doc))\n",
    "    bunsetsu_span = ginza.bunsetu_span(token)\n",
    "    # return [t.lemma_ if t not in bunsetsu_head_list else t.pos_ for t in bunsetsu_span]\n",
    "    # if token not in bunsetsu_head_list:\n",
    "    #     return token.lemma_\n",
    "    # else: return token.pos_\n",
    "    for t in bunsetsu_span:\n",
    "        # skip punctuation\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        # if t  the head word of the bunsetsu, or if it depends on a right token from the same bunsetsu\n",
    "        if ginza.is_bunsetu_head(t) or (ginza.head(t) == token and t in ginza.lefts(token)):\n",
    "            result.append(t.pos_)\n",
    "        else: result.append(t.lemma_)\n",
    "    return \"+\".join(result)\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def build_nltk_tree_from_ginza_rec(token, bunsetsu_tokens, node_repr):\n",
    "    # if empty, just print token\n",
    "    #print(token, list(token.children))\n",
    "    # to avoid repetitions, use only one bunsetsu once\n",
    "    selected_children = intersection(token.children, bunsetsu_tokens)\n",
    "\n",
    "    if not selected_children:\n",
    "        return node_repr(token)\n",
    "    \n",
    "    # else recursively do it again\n",
    "    # ordering problem, this way they are ordered\n",
    "    children_trees = [build_nltk_tree_from_ginza_rec(child_token, bunsetsu_tokens, node_repr=node_repr)\n",
    "                       for child_token in selected_children]\n",
    "    return Tree(node_repr(token), children_trees)\n",
    "\n",
    "def build_nltk_tree_from_ginza(sentence: Union[str,spacy.tokens.doc.Doc, spacy.tokens.span.Span], nlp=None,\n",
    "                                node_repr:callable(spacy.tokens.token.Token)=bunsetsu_sub):\n",
    "    \"\"\"Given a string, parses it and returns a nltk Tree. It will then be used by the fastkassim method.\n",
    "    Otherwise, a doc (span) that has already been sentencised can be passed. Otherwise it will be sentencised.\n",
    "    Input:\n",
    "    sentence: \n",
    "    nlp: Optional\n",
    "    node_repr: the function to apply to each token (eg use the full bunsetsu or do substitutions)\n",
    "\n",
    "    returns: nlkt.Tree object\"\"\"\n",
    "    doc = sentence\n",
    "    if type(sentence) is str:\n",
    "        if nlp is None:\n",
    "            raise ValueError(f\"If string object you need to pass an nlp spacy object, not {nlp}\")\n",
    "        doc = nlp(sentence)\n",
    "        doc = list(doc.sents)[0]\n",
    "    elif type(sentence) is spacy.tokens.doc.Doc:\n",
    "        doc = list(doc.sents)[0]\n",
    "    \n",
    "    root = doc.root\n",
    "    spans = ginza.bunsetu_spans(doc)\n",
    "    bunsetsu_tokens = [span.root for span in spans]\n",
    "    return build_nltk_tree_from_ginza_rec(token=root, bunsetsu_tokens=bunsetsu_tokens, node_repr=node_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'ja_ginza_electra' (5.1.3) was trained with spaCy v3.2.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('ja_ginza_electra')\n",
    "FastKassim = fkassim.FastKassim(fkassim.FastKassim.LTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = 'This is an english sentence.'\n",
    "s1 = '田中さんはカレーを食べました。' # this should be similar to the second\n",
    "s2 = '犬は水を飲んだ。' # this should be similar to the first\n",
    "s3 = '見たことがないです。' # this should be not similar to anything\n",
    "s4 = '可愛い田中さんはカレーを食べました。' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"72px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,176.0,72.0\" width=\"176px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB+ます+た</text></svg><svg width=\"63.6364%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PROPN+NOUN+は</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.8182%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"36.3636%\" x=\"63.6364%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+を</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.8182%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('VERB+ます+た', ['PROPN+NOUN+は', 'NOUN+を'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_nltk_tree_from_ginza(s1, nlp=nlp, node_repr=bunsetsu_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       VERB+ます+た             \n",
      "   ________|__________        \n",
      "  |              PROPN+NOUN+は\n",
      "  |                   |       \n",
      "NOUN+を               ADJ     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_nltk_tree_from_ginza(s4, nlp=nlp, node_repr=bunsetsu_sub).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fkassim_custom_tokenizer_spacy(sentence: str):\n",
    "    l = []\n",
    "    for t in nlp.tokenizer(sentence):\n",
    "        l.append(t.text)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       VERB+ます+た             \n",
      "   ________|__________        \n",
      "  |              PROPN+NOUN+は\n",
      "  |                   |       \n",
      "NOUN+を               ADJ     \n",
      "\n",
      "       VERB+ます+た             \n",
      "   ________|__________        \n",
      "  |              PROPN+NOUN+は\n",
      "  |                   |       \n",
      "NOUN+を               ADJ     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1_parsed = build_nltk_tree_from_ginza('可愛い田中さんはカレーを食べました。', nlp=nlp)\n",
    "s1_parsed.pretty_print()\n",
    "s1_parsed = build_nltk_tree_from_ginza('カレーを可愛い田中さんは食べました。', nlp=nlp)\n",
    "s1_parsed.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('VERB_ROOT', [Tree('NOUN_nsubj', ['PROPN_compound', 'ADP_case']), Tree('NOUN_obj', ['ADP_case']), 'AUX_aux', 'AUX_aux', 'PUNCT_punct'])]\n",
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['This'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['an']), Tree('JJ', ['english']), Tree('NN', ['sentence'])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "# see how it gets tokenized\n",
    "# it wants a list of token string/text\n",
    "eng_parsed = FastKassim.parse_document(s0)\n",
    "# s1_parsed = FastKassim.parse_document(s1, tokenizer=fkassim_custom_tokenizer_spacy)\n",
    "# s2_parsed = FastKassim.parse_document(s2, tokenizer=fkassim_custom_tokenizer_spacy)\n",
    "# s3_parsed = FastKassim.parse_document(s3, tokenizer=fkassim_custom_tokenizer_spacy)\n",
    "s1_parsed = build_nltk_tree_from_ginza(s1, nlp=nlp)\n",
    "s2_parsed = build_nltk_tree_from_ginza(s2, nlp=nlp)\n",
    "s3_parsed = build_nltk_tree_from_ginza(s3, nlp=nlp)\n",
    "s4_parsed = build_nltk_tree_from_ginza(s4, nlp=nlp)\n",
    "\n",
    "# second test\n",
    "s1_parsed = nltk_spacy_tree(s1, nlp=nlp)\n",
    "s2_parsed = nltk_spacy_tree(s2, nlp=nlp)\n",
    "s3_parsed = nltk_spacy_tree(s3, nlp=nlp)\n",
    "s4_parsed = nltk_spacy_tree(s4, nlp=nlp)\n",
    "print(s1_parsed)\n",
    "print(eng_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ROOT                         \n",
      "          |                            \n",
      "          S                           \n",
      "  ________|_________________________   \n",
      " |             VP                   | \n",
      " |     ________|_____               |  \n",
      " NP   |              NP             | \n",
      " |    |    __________|_______       |  \n",
      " DT  VBZ  DT         JJ      NN     . \n",
      " |    |   |          |       |      |  \n",
      "This  is  an      english sentence  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eng_parsed[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp('ciao')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,536.0,120.0\" width=\"536px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB_ROOT</text></svg><svg width=\"38.806%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN_nsubj</text></svg><svg width=\"61.5385%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PROPN_compound</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.7692%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"38.4615%\" x=\"61.5385%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADP_case</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"80.7692%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.403%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"14.9254%\" x=\"38.806%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN_obj</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADP_case</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.2687%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.4328%\" x=\"53.7313%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">AUX_aux</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.4478%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.4328%\" x=\"67.1642%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">AUX_aux</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.8806%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"19.403%\" x=\"80.597%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PUNCT_punct</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.2985%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('VERB_ROOT', [Tree('NOUN_nsubj', ['PROPN_compound', 'ADP_case']), Tree('NOUN_obj', ['ADP_case']), 'AUX_aux', 'AUX_aux', 'PUNCT_punct'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_parsed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "田中さんはカレーを食べました。 犬は水を飲んだ。 0.0\n",
      "田中さんはカレーを食べました。 見たことがないです。 0.0\n",
      "犬は水を飲んだ。 見たことがないです。 0.2279601015231132\n",
      "田中さんはカレーを食べました。 可愛い田中さんはカレーを食べました。 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(s1, s2, FastKassim.compute_similarity_preparsed(s1_parsed, s2_parsed))\n",
    "print(s1, s3, FastKassim.compute_similarity_preparsed(s1_parsed, s3_parsed))\n",
    "print(s2, s3, FastKassim.compute_similarity_preparsed(s2_parsed, s3_parsed))\n",
    "print(s1, s4, FastKassim.compute_similarity_preparsed(s1_parsed, s4_parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 VERB_ROOT                                             \n",
      "    _________________|____________________________________________      \n",
      "   |       |         |                     NOUN_nsubj          NOUN_obj\n",
      "   |       |         |             ____________|_________         |     \n",
      "AUX_aux AUX_aux PUNCT_punct PROPN_compound            ADP_case ADP_case\n",
      "\n",
      "         VERB_ROOT                     \n",
      "    _________|____________________      \n",
      "   |         |      NOUN_nsubj NOUN_obj\n",
      "   |         |          |         |     \n",
      "AUX_aux PUNCT_punct  ADP_case  ADP_case\n",
      "\n",
      "                            VERB_ROOT                                 \n",
      "    ____________________________|________________________________      \n",
      "   |       |         |                  NOUN_nsubj            NOUN_obj\n",
      "   |       |         |           ___________|___________         |     \n",
      "AUX_aux AUX_aux PUNCT_punct  ADJ_acl  PROPN_compound ADP_case ADP_case\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1_parsed[0].pretty_print()\n",
    "s2_parsed[0].pretty_print()\n",
    "s4_parsed[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing tree conversion\n",
    "\n",
    "# Parse the sentence using SpaCy\n",
    "doc = nlp('日本の四季は非常に美しく、春には桜の花が満開に咲き、夏には太陽が輝き、秋には紅葉が山々を彩り、冬には雪が積もり、その美しさに心が癒されます。')\n",
    "doc = nlp('可愛い田中さんはカレーを食べました。')\n",
    "# Method to convert SpaCy dependency tree to NLTK Tree format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_token\n",
    "# spans = ginza.bunsetu_spans(doc)\n",
    "# bunsetsu_tokens = [span.root for span in spans]\n",
    "# for token in bunsetsu_tokens:\n",
    "#     print(ginza.bunsetu(token))\n",
    "#     #print(token.children, bunsetsu_tokens)\n",
    "#     print(intersection(token.children, bunsetsu_tokens))\n",
    "#     #print(list(token.ancestors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "可愛い田中さんはカレーを食べました。"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREE norm tests\n",
    "\n",
    "doc = list(doc.sents)[0]\n",
    "root = doc.root\n",
    "spans = ginza.bunsetu_spans(doc)\n",
    "bunsetsu_tokens = [span.root for span in spans]\n",
    "token = doc.root\n",
    "#selected_children = intersection(token.children, bunsetsu_tokens)\n",
    "#print(selected_children)\n",
    "#if not selected_children:\n",
    "#    print(bunsetsu_sub(token))\n",
    "    \n",
    "# else recursively do it again\n",
    "# ordering problem, this way they are ordered\n",
    "children_trees = Tree.fromlist(['sto', ['cazzo']])\n",
    "tree = Tree(bunsetsu_sub(token), children_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"360px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,960.0,360.0\" width=\"960px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB+れる+ます</text></svg><svg width=\"85.8333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB</text></svg><svg width=\"82.5243%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB</text></svg><svg width=\"69.4118%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB</text></svg><svg width=\"69.4915%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VERB</text></svg><svg width=\"39.0244%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ</text></svg><svg width=\"56.25%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+は</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PROPN+の</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"43.75%\" x=\"56.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ+だ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.125%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.5122%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"24.3902%\" x=\"39.0244%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+に+は</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.2195%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"19.5122%\" x=\"63.4146%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+が</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+の</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.1707%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"17.0732%\" x=\"82.9268%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ+に</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.4634%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.7458%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"16.9492%\" x=\"69.4915%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+に+は</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.9661%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.5593%\" x=\"86.4407%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+が</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.2203%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.7059%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"11.7647%\" x=\"69.4118%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+に+は</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.2941%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.41176%\" x=\"81.1765%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+が</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.8824%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.41176%\" x=\"90.5882%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+を</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.2941%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.2621%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.70874%\" x=\"82.5243%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+に+は</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.3786%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.76699%\" x=\"92.233%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+が</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.1165%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.9167%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"85.8333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ADJ+さ+に</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DET</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.5833%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.66667%\" x=\"93.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NOUN+が</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.6667%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('VERB+れる+ます', [Tree('VERB', [Tree('VERB', [Tree('VERB', [Tree('VERB', [Tree('ADJ', [Tree('NOUN+は', ['PROPN+の']), 'ADJ+だ']), 'NOUN+に+は', Tree('NOUN+が', ['NOUN+の']), 'ADJ+に']), 'NOUN+に+は', 'NOUN+が']), 'NOUN+に+は', 'NOUN+が', 'NOUN+を']), 'NOUN+に+は', 'NOUN+が']), Tree('ADJ+さ+に', ['DET']), 'NOUN+が'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_tree = '日本の四季は非常に美しく、春には桜の花が満開に咲き、夏には太陽が輝き、秋には紅葉が山々を彩り、冬には雪が積もり、その美しさに心が癒されます。'\n",
    "tree = build_nltk_tree_from_ginza(s_tree, nlp=nlp, node_repr=bunsetsu_sub)\n",
    "#tree.chomsky_normal_form()\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunsetsu_sub(token):\n",
    "    \"\"\"Performs substitution similar to Tolmachev et al.\"\"\"\n",
    "    result = []\n",
    "    # bunsetsu_head_list = list(ginza.bunsetu_head_tokens(token.doc))\n",
    "    bunsetsu_span = ginza.bunsetu_span(token)\n",
    "    # return [t.lemma_ if t not in bunsetsu_head_list else t.pos_ for t in bunsetsu_span]\n",
    "    # if token not in bunsetsu_head_list:\n",
    "    #     return token.lemma_\n",
    "    # else: return token.pos_\n",
    "    for t in bunsetsu_span:\n",
    "        # skip punctuation\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        # if t  the head word of the bunsetsu, or if it depends on a right token from the same bunsetsu\n",
    "        if ginza.is_bunsetu_head(t) or (ginza.head(t) == token and t in ginza.lefts(token)):\n",
    "            result.append(t.pos_)\n",
    "        else: result.append(t.lemma_)\n",
    "    return \"+\".join(result)\n",
    "\n",
    "def nltk_spacy_tree(sent, nlp):\n",
    "    \"\"\"\n",
    "    Visualize the SpaCy dependency tree with nltk.tree\n",
    "    \"\"\"\n",
    "    doc = nlp(sent)\n",
    "    def token_format(token):\n",
    "        return \"_\".join([token.pos_, token.dep_])\n",
    "\n",
    "    def to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return Tree(token_format(node),\n",
    "                       [to_nltk_tree(child) \n",
    "                        for child in node.children]\n",
    "                   )\n",
    "        else:\n",
    "            return token_format(node)\n",
    "\n",
    "    tree = [to_nltk_tree(sent.root) for sent in doc.sents]\n",
    "    # The first item in the list is the full tree\n",
    "    return tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 VERB_ROOT                                             \n",
      "    _________________|____________________________________________      \n",
      "   |       |         |                     NOUN_nsubj          NOUN_obj\n",
      "   |       |         |             ____________|_________         |     \n",
      "AUX_aux AUX_aux PUNCT_punct PROPN_compound            ADP_case ADP_case\n",
      "\n",
      "                 VERB_ROOT                                             \n",
      "    _________________|__________________________________                \n",
      "   |       |         |      NOUN_obj                NOUN_nsubj         \n",
      "   |       |         |         |            ____________|_________      \n",
      "AUX_aux AUX_aux PUNCT_punct ADP_case PROPN_compound            ADP_case\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = '田中さんはカレーを食べました。' \n",
    "   \n",
    "tree1 = nltk_spacy_tree(doc, nlp)\n",
    "tree1.pretty_print()\n",
    "\n",
    "doc = 'カレーを田中さんは食べました。' \n",
    "   \n",
    "tree2 = nltk_spacy_tree(doc, nlp)\n",
    "tree2.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB_ROOT'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('VERB_ROOT', [Tree('NOUN_nsubj', ['PROPN_compound', 'ADP_case']), Tree('NOUN_obj', ['ADP_case']), 'AUX_aux', 'AUX_aux', 'PUNCT_punct'])]\n",
      "[Tree('VERB_ROOT', [Tree('NOUN_obj', ['ADP_case']), Tree('NOUN_nsubj', ['PROPN_compound', 'ADP_case']), 'AUX_aux', 'AUX_aux', 'PUNCT_punct'])]\n"
     ]
    }
   ],
   "source": [
    "print(tree1)\n",
    "print(tree2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
