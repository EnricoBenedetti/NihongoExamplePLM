{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import *\n",
    "import generation\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import spacy\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'ja_ginza' (5.1.3) was trained with spaCy v3.2.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ja_ginza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write 5 beginner example sentences in japanese, that must contain the word \"初め\" used in a similar sense as \"十一月の初めだった。\". following are 5 diverse sentences that must use \"初め\": '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for testing\n",
    "k = 5\n",
    "target_word = '初め'\n",
    "target_level = 'beginner'\n",
    "context_sentence = \"十一月の初めだった。\"\n",
    "prompt = f'write {k} {target_level} example sentences in japanese, that must contain the word \"{target_word}\" used in a similar sense as \"{context_sentence}\". following are {k} diverse sentences that must use \"{target_word}\": '\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='1. 彼との意見が合わず、議論が続いている。\\n2. 予定が合わず、会議に参加できなかった。\\n3. 彼女の要求と私の能力が合わず、仕事を断らざるを得なかった。\\n4. 彼の行動が社会のルールに合わず、非難されている。\\n5. この問題は私たちの専門分野には合わないので、他の専門家に相談した方がいい。', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  temperature = 0.2,\n",
    "  seed = 42,\n",
    "  n = 2,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Provide sentences in Japanese in a numbered list, without any translation or romaji. Use the provided kanji form of the word.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['彼との意見が合わず、議論が続いている。',\n",
       " '予定が合わず、会議に参加できなかった。',\n",
       " '彼女の要求と私の能力が合わず、仕事を断らざるを得なかった。',\n",
       " '彼の行動が社会のルールに合わず、非難されている。',\n",
       " 'この問題は私たちの専門分野には合わないので、他の専門家に相談した方がいい。',\n",
       " 'この二つのパズルのピースは完璧に合っている。',\n",
       " '彼の意見と私の意見は全く合わない。',\n",
       " '今日の天気予報は外れていて、実際の天気と合わない。',\n",
       " '彼女の性格と私の性格はとても合う。',\n",
       " 'この料理は辛さと甘さが絶妙に合っている。']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = \"\".join([choice.message.content for choice in completion.choices])\n",
    "utils.extract_sentences_with_target_word(target_word, generated_text, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['彼は新しい仕事の初めに緊張していた。',\n",
       " '私たちは春の初めに桜を見に行く予定です。',\n",
       " '学校の初めは新しい友達を作るチャンスです。',\n",
       " '旅行の初めに地図を確認しましょう。',\n",
       " '一年の初めに目標を立てることは重要です。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = completion.choices[0].message.content\n",
    "generated_text\n",
    "utils.extract_sentences_with_target_word(target_word, generated_text, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/data/enrico_benedetti/nihongoexample/code/scripts/generation_gpt.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bieyasu/data/enrico_benedetti/nihongoexample/code/scripts/generation_gpt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bieyasu/data/enrico_benedetti/nihongoexample/code/scripts/generation_gpt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m             candidates\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/data/enrico_benedetti/nihongoexample/evaluation/outputs/experiments/llmjp_\u001b[39m\u001b[39m{\u001b[39;00mtarget_word\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mtarget_level_real\u001b[39m}\u001b[39;00m\u001b[39m_.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bieyasu/data/enrico_benedetti/nihongoexample/code/scripts/generation_gpt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvg number of candidates \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(candidates_len)\u001b[39m}\u001b[39;00m\u001b[39m, min: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmin(candidates_len)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2946\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2829\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[1;32m   2830\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamin\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2831\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2832\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m \u001b[39m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2834\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[39m    6\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2946\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mminimum, \u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2947\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# run on everything\n",
    "dev_only = False\n",
    "log = utils.get_logger()\n",
    "df_target = pd.read_csv('../../data/targets/target_words.csv')\n",
    "#df_target_test = df_target[df_target['is_test'] == False]\n",
    "# then it should be\n",
    "# df_target_test = df_target[df_target['is_test'] == True]\n",
    "# and finally (everything)\n",
    "df_target_test = df_target\n",
    "k = 5\n",
    "target_levels = ['beginner', 'intermediate', 'advanced']\n",
    "target_levels_real = ['N5', 'N3', 'N1']\n",
    "candidates_len = []\n",
    "for target_level, target_level_real in zip(target_levels, target_levels_real):\n",
    "    for i, data in df_target_test.iterrows():\n",
    "\n",
    "        target_word = data['target_word']\n",
    "        context_sentence = data['context_sentence']\n",
    "        ## just call if file does not exists ->\n",
    "        if os.path.exists(f\"/data/enrico_benedetti/nihongoexample/evaluation/outputs/generation/chatgpt/{target_word}_{target_level_real}_.csv\"):\n",
    "            continue\n",
    "        \n",
    "        prompt = f'write {k} {target_level} example sentences in japanese, that must contain the word \"{target_word}\" used in a similar sense as \"{context_sentence}\". following are {k} diverse sentences that must use \"{target_word}\": '\n",
    "        candidates_list = [context_sentence]\n",
    "        cycle_max = 0\n",
    "        while len(candidates_list) -1 < k and cycle_max < 1:\n",
    "            # call the API\n",
    "\n",
    "                \n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature = 1.0,\n",
    "                n = 2, # give 2 tries\n",
    "                seed = 42,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Provide sentences in Japanese in a numbered list, without any translation or romaji.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                    ])\n",
    "            # get the generated text and get the sentences\n",
    "            #generated_text = completion.choices[0].message.content\n",
    "            generated_text = \"\".join([choice.message.content for choice in completion.choices])\n",
    "            # debug the gen text for reference\n",
    "            log.debug(f\"{prompt}, {generated_text}\")\n",
    "            # add the generated sentences\n",
    "            candidates_list += utils.extract_sentences_with_target_word(target_word, generated_text, nlp)\n",
    "            candidates = pd.DataFrame({'sentence': candidates_list})\n",
    "            candidates.drop_duplicates(subset='sentence', inplace=True)\n",
    "            candidates_list = candidates['sentence'].to_list()\n",
    "            cycle_max += 1\n",
    "        #end while\n",
    "        time.sleep(60)\n",
    "        #save file\n",
    "        candidates['target_word'] = target_word\n",
    "        candidates['context_sentence'] = context_sentence\n",
    "        candidates['target_level'] = target_level_real\n",
    "        candidates_len.append(len(candidates_list)-1)\n",
    "        log.info(f\"{target_word}, {target_level}, {len(candidates_list)-1}, {cycle_max}\")\n",
    "        if len(candidates_list) -1 < k:\n",
    "            log.error(f\"{target_word}, {target_level} has only {len(candidates_list) -1} sentences out of {k}\")\n",
    "        if not dev_only:\n",
    "            candidates.to_csv(f\"/data/enrico_benedetti/nihongoexample/evaluation/outputs/generation/chatgpt/{target_word}_{target_level_real}_.csv\", index=False)\n",
    "        else:\n",
    "            candidates.to_csv(f\"/data/enrico_benedetti/nihongoexample/evaluation/outputs/experiments/llmjp_{target_word}_{target_level_real}_.csv\", index=False)\n",
    "        \n",
    "\n",
    "log.debug(f\"Avg number of candidates {np.mean(candidates_len)}, min: {np.min(candidates_len)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
