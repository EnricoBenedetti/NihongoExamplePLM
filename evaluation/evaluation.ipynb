{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"sacrebleu[ja]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from itertools import combinations\n",
    "import fkassim.FastKassim as fkassim\n",
    "import ginza\n",
    "from ginza import lemma_, bunsetu, sub_phrases\n",
    "import spacy\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "#from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import ginza\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "#import sacrebleu\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_length_measures(sentences: list):\n",
    "    \"\"\"Given the sentence list (of strings), computes the average sentence length. In characters\n",
    "    Should I also do it with tokens/words/bunsetsus?\n",
    "    :return: avg_length, stddev\"\"\"\n",
    "    sentence_length_list = [len(sent) for sent in sentences]\n",
    "    return np.mean(sentence_length_list), np.std(sentence_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from he et al. https://github.com/NLPCode/CDEG/blob/master/evaluations/automatic_evaluation.py\n",
    "def compute_distances_and_entropy(tokenized_sentences_list, ngrams=[1, 2, 3, 4], num_tokens=0):\n",
    "        \"\"\"\n",
    "        this function is used to calculate the percentage of unique n-grams to measure the generation diversity.\n",
    "        this function is also used to calculate entropy to measure the generation diversity.\n",
    "        :param tokenized_sentences_list:\n",
    "        :param ngrams:\n",
    "        :param num_tokens:\n",
    "        :return: the percentage of unique n-grams and entropy of their distribution\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        entropies = []\n",
    "        if num_tokens > 0:\n",
    "            cur_num = 0\n",
    "            new_tokenized_sentences_list = []\n",
    "            for tokenized_sentence in tokenized_sentences_list:\n",
    "                cur_num += len(tokenized_sentence)\n",
    "                new_tokenized_sentences_list.append(tokenized_sentence)\n",
    "                if cur_num >= num_tokens:\n",
    "                    break\n",
    "            tokenized_sentences_list = new_tokenized_sentences_list\n",
    "\n",
    "        for n in ngrams:\n",
    "            # calculate (n-gram, frequency) pairs\n",
    "            ngram_fdist = nltk.FreqDist()\n",
    "            for tokens in tokenized_sentences_list:\n",
    "                ngrams = nltk.ngrams(tokens, n)\n",
    "                ngram_fdist.update(ngrams)\n",
    "            unique = ngram_fdist.B()  # the number of unique ngrams\n",
    "            total = ngram_fdist.N()  # the number of ngrams\n",
    "            distances.append(unique * 1.0 / total)\n",
    "            # calculate entropies\n",
    "            ans = 0.0\n",
    "            for k, v in ngram_fdist.items():\n",
    "                ans += v * np.log(v * 1.0 / total)\n",
    "            ans = -ans / total\n",
    "            entropies.append(ans)\n",
    "        return distances, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_self_BLEU(tokenized_sentence_list, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    \"\"\"Computes BLEU score considering each sentence as hypotesis and the others as references, then averages the scores.\n",
    "    input: the list of sentences,\n",
    "    weights: the weights to give n-grams\n",
    "    ---\n",
    "    output: the bleu score.\"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for i in range(len(tokenized_sentence_list)):\n",
    "        hypothesis = tokenized_sentence_list[i]\n",
    "        references = tokenized_sentence_list[:i] + tokenized_sentence_list[i+1:]\n",
    "        score = sentence_bleu(hypothesis=hypothesis, references=references, weights=weights)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunsetsu_sub(token):\n",
    "    \"\"\"Performs substitution similar to Tolmachev et al.\"\"\"\n",
    "    result = []\n",
    "    # bunsetsu_head_list = list(ginza.bunsetu_head_tokens(token.doc))\n",
    "    bunsetsu_span = ginza.bunsetu_span(token)\n",
    "    # return [t.lemma_ if t not in bunsetsu_head_list else t.pos_ for t in bunsetsu_span]\n",
    "    # if token not in bunsetsu_head_list:\n",
    "    #     return token.lemma_\n",
    "    # else: return token.pos_\n",
    "    for t in bunsetsu_span:\n",
    "        # skip punctuation\n",
    "        if t.is_punct:\n",
    "            continue\n",
    "        # if t  the head word of the bunsetsu, or if it depends on a right token from the same bunsetsu\n",
    "        if ginza.is_bunsetu_head(t) or (ginza.head(t) == token and t in ginza.lefts(token)):\n",
    "            result.append(t.pos_)\n",
    "        else: result.append(t.lemma_)\n",
    "    return \"+\".join(result)\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def build_nltk_tree_from_ginza_rec(token, bunsetsu_tokens, node_repr):\n",
    "    # if empty, just print token\n",
    "    #print(token, list(token.children))\n",
    "    # to avoid repetitions, use only one bunsetsu once\n",
    "    selected_children = intersection(token.children, bunsetsu_tokens)\n",
    "\n",
    "    if not selected_children:\n",
    "        return node_repr(token)\n",
    "    \n",
    "    # else recursively do it again\n",
    "    # ordering problem, this way they are ordered\n",
    "    children_trees = [build_nltk_tree_from_ginza_rec(child_token, bunsetsu_tokens, node_repr=node_repr)\n",
    "                       for child_token in selected_children]\n",
    "    return Tree(node_repr(token), children_trees)\n",
    "\n",
    "def build_nltk_tree_from_ginza(sentence: Union[str,spacy.tokens.doc.Doc, spacy.tokens.span.Span], nlp=None,\n",
    "                                node_repr:callable(spacy.tokens.token.Token)=bunsetsu_sub):\n",
    "    \"\"\"Given a string, parses it and returns a nltk Tree. It will then be used by the fastkassim method.\n",
    "    Otherwise, a doc (span) that has already been sentencised can be passed. Otherwise it will be sentencised.\n",
    "    Input:\n",
    "    sentence: \n",
    "    nlp: Optional\n",
    "    node_repr: the function to apply to each token (eg use the full bunsetsu or do substitutions)\n",
    "\n",
    "    returns: nlkt.Tree object\"\"\"\n",
    "    doc = sentence\n",
    "    if type(sentence) is str:\n",
    "        if nlp is None:\n",
    "            raise ValueError(f\"If string object you need to pass an nlp spacy object, not {nlp}\")\n",
    "        doc = nlp(sentence)\n",
    "        doc = list(doc.sents)[0]\n",
    "    elif type(sentence) is spacy.tokens.doc.Doc:\n",
    "        doc = list(doc.sents)[0]\n",
    "    \n",
    "    root = doc.root\n",
    "    spans = ginza.bunsetu_spans(doc)\n",
    "    bunsetsu_tokens = [span.root for span in spans]\n",
    "    return build_nltk_tree_from_ginza_rec(token=root, bunsetsu_tokens=bunsetsu_tokens, node_repr=node_repr)\n",
    "\n",
    "def nltk_spacy_tree(sent, nlp):\n",
    "    \"\"\"\n",
    "    Visualize the SpaCy dependency tree with nltk.tree\n",
    "    \"\"\"\n",
    "    doc = nlp(sent)\n",
    "    def token_format(token):\n",
    "        return \"_\".join([token.pos_, token.dep_])\n",
    "\n",
    "    def to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return Tree(token_format(node),\n",
    "                       [to_nltk_tree(child) \n",
    "                        for child in node.children]\n",
    "                   )\n",
    "        else:\n",
    "            return token_format(node)\n",
    "\n",
    "    tree = [to_nltk_tree(sent.root) for sent in doc.sents]\n",
    "    # The first item in the list is the full tree\n",
    "    return tree[0]\n",
    "\n",
    "\n",
    "# now we need to do comparisons for lists of sentences\n",
    "def compute_fastkassim_similarity(sentences: list, nlp):\n",
    "    \"\"\"Given a a list of sentences, will produce the trees and perform pairwise comparisons and take the avg\"\"\"\n",
    "    FastKassim = fkassim.FastKassim(fkassim.FastKassim.LTK)\n",
    "    s_trees = []\n",
    "    sims = []\n",
    "\n",
    "    for s in sentences:\n",
    "        s_trees.append(nltk_spacy_tree(s, nlp=nlp))\n",
    "    \n",
    "    for s1, s2 in combinations(s_trees,2):\n",
    "        sims.append(FastKassim.compute_similarity_preparsed(s1,s2))\n",
    "    return np.mean(sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'ja_ginza' (5.1.3) was trained with spaCy v3.2.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# use spacy to get different tokenizations\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['私', 'は', '毎朝', 'パン', 'を', '食べる', '。'], ['猫', 'が', '魚', 'を', '食べる', 'の', 'が', '好き', 'です', '。'], ['彼女', 'は', 'レストラン', 'で', 'ピザ', 'を', '食べる', 'つもり', 'です', '。'], ['昨日', '、', '私たち', 'は', '外', 'で', 'ラーメン', 'を', '食べ', 'まし', 'た', '。'], ['彼', 'は', '急い', 'で', 'サンドイッチ', 'を', '食べる', '。'], ['あの', '子', 'は', 'ケーキ', 'を', '食べる', 'の', 'が', '得意', 'です', '。'], ['私達', 'は', '一緒', 'に', '夕食', 'を', '食べる', 'こと', 'に', 'し', 'まし', 'た', '。'], ['彼女', 'は', '健康', 'の', 'ため', 'に', 'サラダ', 'を', '食べる', '。'], ['犬', 'は', '何', 'で', 'も', '食べる', '。'], ['夏', 'に', 'は', 'よく', 'アイスクリーム', 'を', '食べる', '。']]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences with the word 'taberu' (食べる)\n",
    "sentences = [\n",
    "    \"私は毎朝パンを食べる。\",  # I eat bread every morning.\n",
    "    \"猫が魚を食べるのが好きです。\",  # The cat likes to eat fish.\n",
    "    \"彼女はレストランでピザを食べるつもりです。\",  # She plans to eat pizza at the restaurant.\n",
    "    \"昨日、私たちは外でラーメンを食べました。\",  # Yesterday, we ate ramen outside.\n",
    "    \"彼は急いでサンドイッチを食べる。\",  # He eats a sandwich in a hurry.\n",
    "    \"あの子はケーキを食べるのが得意です。\",  # That child is good at eating cake.\n",
    "    \"私達は一緒に夕食を食べることにしました。\",  # We decided to eat dinner together.\n",
    "    \"彼女は健康のためにサラダを食べる。\",  # She eats salad for her health.\n",
    "    \"犬は何でも食べる。\",  # Dogs eat anything.\n",
    "    \"夏にはよくアイスクリームを食べる。\"  # I often eat ice cream in the summer.\n",
    "]\n",
    "\n",
    "#sentences = [\"犯人が捕まったという話は滅多に聞かない。\", \"私は毎朝パンを食べる。\"]\n",
    "\n",
    "def get_tokenized_sentences(sentences: str) -> list:\n",
    "    \"\"\"returns a list tokenized sentences (list of list of str)\"\"\"\n",
    "    return [[token.text for token in nlp.tokenizer(sent)] for sent in sentences]\n",
    "# Tokenizing the sentences (assuming each sentence is already properly segmented in Japanese)\n",
    "# keep only 'morphologizer'\n",
    "#sentences_docs = nlp.pipe(sentences, disable=['tok2vec','parser','senter','attribute_ruler','ner'])\n",
    "#sentences_docs = nlp.pipe(sentences)\n",
    "tokenized_sentences = get_tokenized_sentences(sentences)\n",
    "print(tokenized_sentences)\n",
    "# using nlp.tokenizer is faster.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init test 0.7583333333333333\n",
      "no reorder 0.7677973956662546\n",
      "reorder 1.0\n"
     ]
    }
   ],
   "source": [
    "kassim_sentences =['田中さんはカレーを食べました。', # this should be similar to the second\n",
    "    '犬は水を飲んだ。' ,# this should be similar to the first\n",
    "    '見たことがないです。' ,# this should be not similar to anything\n",
    "    '可愛い田中さんはカレーを食べました。' ]\n",
    "kassim_sentences_no_reorder =['可愛い田中さんはカレーを食べました。', \n",
    "    'カレーを可愛い田中さんは食べました。' ]\n",
    "kassim_sentences_reorder =['田中さんはカレーを食べました。', \n",
    "    'カレーを田中さんは食べました。' ]\n",
    "print('init test', compute_fastkassim_similarity(kassim_sentences, nlp))\n",
    "print('no reorder',compute_fastkassim_similarity(kassim_sentences_no_reorder, nlp))\n",
    "print('reorder',compute_fastkassim_similarity(kassim_sentences_reorder, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': False, 'sigma': 1, 'lmbda': 0.4, 'use_new_delta': True}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastKassim = fkassim.FastKassim(fkassim.FastKassim.LTK)\n",
    "FastKassim.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg length 16.3 +- 3.7429934544425802\n",
      "self bleu 0.06023724351635462\n",
      "distances (unique ngrams) - entropies: ([0.5, 0.7906976744186046, 0.9210526315789473, 0.9848484848484849], [3.444685034297525, 4.048416708262656, 4.203048333341079, 4.168650282009455])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# calculating avg sentence length\n",
    "avg_length, std = compute_length_measures(sentences)\n",
    "print('avg length', avg_length, '+-', std)\n",
    "# Calculating self-BLEU\n",
    "self_bleu_score = compute_self_BLEU(tokenized_sentences)\n",
    "print('self bleu', self_bleu_score)\n",
    "# calculating unique n-grams and entropies\n",
    "ngrams_entropies = compute_distances_and_entropy(tokenized_sentences)\n",
    "print('distances (unique ngrams) - entropies:', ngrams_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.625, 0.8333333333333334, 1.0, 1.0],\n",
       " [1.559581156259877,\n",
       "  1.5607104090414063,\n",
       "  1.3862943611198906,\n",
       "  0.6931471805599453])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_test = [['sono', 'ehy', \"ciao\", 'mamma'],['mamma', 'ehy','ciao', '.']]\n",
    "compute_distances_and_entropy(ngrams_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELFBLEU results (lower is better diversity) \n",
      "diverse: 0.1984155389876064 similar: 0.21653923593362454 all: 0.25870440709309006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/enrico_benedetti/anaconda3/envs/nlp_env/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# test using diverse sentences\n",
    "\n",
    "def test_self_bleu():\n",
    "    \"\"\"Ten examples. It should be that the diverse sentences have lower bleu score, and the opposite for the similar\"\"\"\n",
    "    diverse_sentences = [\n",
    "        \"毎朝、鳥の声を聞くのが日課です。\", #(Listening to the birds' voices every morning is my daily routine.)\n",
    "        \"海の音を聞くと、心が落ち着きます。\", #(Hearing the sound of the sea calms my mind.)\n",
    "        \"彼はラジオでジャズを聞くのが好きです。\", #(He likes listening to jazz on the radio.)\n",
    "        \"私たちは山で風の音を聞きながらキャンプしました。\", #(We camped in the mountains, listening to the sound of the wind.)\n",
    "        \"彼女は異文化について聞くことに興味があります。\", #(She is interested in hearing about different cultures.)\n",
    "        \"その古い物語を聞いて、子供たちはワクワクしました。\", #(The children were excited to hear the old tale.)\n",
    "        \"昨夜、遠くの雷の音を聞いて、驚きました。\", #(Last night, I was surprised to hear the distant thunder.)\n",
    "        \"彼は外国語を聞いてもすぐに理解できます。\", #(He can understand foreign languages just by listening.)\n",
    "        \"私は彼がピアノを弾くのを聞きに行きます。\", #(I'm going to listen to him play the piano.)\n",
    "        \"その詩を聞くたびに、新しい発見があります。\", #(Every time I hear that poem, I discover something new.)\n",
    "    ]\n",
    "    similar_sentences = [\n",
    "        \"彼に明日の予定を聞きました。\",  # I asked him about tomorrow's schedule.            \n",
    "        \"彼女から話を聞きました。\",     # I heard a story from her.\n",
    "        \"友達が彼の意見を聞いた。\",     # My friend asked for his opinion.\n",
    "        \"先生に質問を聞いてみました。\",  # I tried asking the teacher a question.\n",
    "        \"彼らはそのニュースを聞いて驚いた。\",  # They were surprised to hear the news.\n",
    "        \"私は彼のアドバイスを聞くつもりです。\",  # I intend to listen to his advice.\n",
    "        \"兄がその曲を聞いて感動した。\",      # My brother was moved when he listened to that song.\n",
    "        \"彼女は子供たちから話を聞くのが好きです。\",  # She likes to hear stories from the children.\n",
    "        \"彼は先生に授業の詳細を聞きました。\",      # He asked the teacher for details of the lesson.\n",
    "        \"彼らはニュースを聞いて心配している。\"     # They are worried after hearing the news.\n",
    "    ]\n",
    "    #print(len(similar_sentences), len(diverse_sentences))\n",
    "    result_diverse = compute_self_BLEU(get_tokenized_sentences(diverse_sentences))\n",
    "    result_similar = compute_self_BLEU(get_tokenized_sentences(similar_sentences))\n",
    "    result_all = compute_self_BLEU(get_tokenized_sentences(similar_sentences+diverse_sentences))\n",
    "    print('SELFBLEU results (lower is better diversity) \\ndiverse:', result_diverse, 'similar:', result_similar, 'all:', result_all)\n",
    "\n",
    "test_self_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastkassim results on syntactic similarity (lower is better diversity) \n",
      "diverse: 0.48830575843830915 similar: 0.7362962962962963 all: 0.5525249520292133\n"
     ]
    }
   ],
   "source": [
    "# fastkassim test\n",
    "def test_fastkassim():\n",
    "    # need to make them similar with chatgpt or something\n",
    "    # diverse syntactical structures with the word \"taberu\"\n",
    "    diverse_sentences =[\n",
    "    \"食べることは楽しいです。\",  # Eating is fun.\n",
    "    \"このりんごを食べてもいいですか？\",  # May I eat this apple?\n",
    "    \"彼女にケーキを食べさせました。\",  # I let her eat cake.\n",
    "    \"食べた後で、彼は眠りました。\",  # After eating, he slept.\n",
    "    \"食べながら、テレビを見ます。\",  # I watch TV while eating.\n",
    "    \"食べたくないのですが。\",  # I don't want to eat.\n",
    "    \"食べ物を見ると、お腹が空きます。\",  # Seeing food makes me hungry.\n",
    "    \"昨日食べたピザは美味しかったです。\",  # The pizza I ate yesterday was delicious.\n",
    "    \"彼は食べる前に手を洗います。\",  # He washes his hands before eating.\n",
    "    \"食べるのを忘れてしまった。\" ] # I forgot to eat.\n",
    "\n",
    "    # similar syntactical structure with the word \"taberu\" (to eat)\n",
    "    similar_sentences =  [\n",
    "    \"私は毎朝パンを食べます。\",  # I eat bread every morning.\n",
    "    \"彼はよく寿司を食べます。\",  # He often eats sushi.\n",
    "    \"彼女は昨日ケーキを食べました。\",  # She ate cake yesterday.\n",
    "    \"私たちは一緒にリンゴを食べます。\",  # We eat apples together.\n",
    "    \"彼は毎晩スープを食べます。\",  # He eats soup every night.\n",
    "    \"犬は毎日ドッグフードを食べます。\",  # The dog eats dog food every day.\n",
    "    \"彼女は週末にピザを食べます。\",  # She eats pizza on weekends.\n",
    "    \"私は時々サラダを食べます。\",  # I sometimes eat salad.\n",
    "    \"子供たちは昼食にハンバーガーを食べます。\",  # The children eat hamburgers for lunch.\n",
    "    \"彼はたまにチョコレートを食べます。\"] # He occasionally eats chocolate.\n",
    "\n",
    "    result_similar = compute_fastkassim_similarity(similar_sentences, nlp)\n",
    "    result_diverse = compute_fastkassim_similarity(diverse_sentences, nlp)\n",
    "    result_all = compute_fastkassim_similarity(diverse_sentences+similar_sentences, nlp)\n",
    "    print('fastkassim results on syntactic similarity (lower is better diversity) \\ndiverse:', result_diverse, 'similar:', result_similar, 'all:', result_all)\n",
    "\n",
    "test_fastkassim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic parse trees from tolmachev https://megagon.ai/ginza-version-4-0-improving-syntactic-structure-analysis-through-japanese-bunsetsu-phrase-extraction-api-integration/\n",
    "# chatgpt bad code\n",
    "\n",
    "\n",
    "# import spacy\n",
    "# import networkx as nx\n",
    "\n",
    "# class SyntacticSimilarityModel:\n",
    "#     def __init__(self, nlp = None):\n",
    "#         \"\"\"Initialize with nlp object (usually ja_ginza_electra) or (ja_core_news_lg)\"\"\"\n",
    "#         if nlp:\n",
    "#             self.nlp = nlp\n",
    "#         else: self.nlp = spacy.load(\"ja_ginza_electra\")\n",
    "\n",
    "#     def parse_sentence(self, sentence):\n",
    "#         # Parse the sentence to extract its dependency tree and POS tags\n",
    "#         doc = self.nlp(sentence)\n",
    "#         return doc\n",
    "\n",
    "#     def simplify_tree(self, doc):\n",
    "#         # Replace lexical items with POS tags, retain function words\n",
    "#         # Implementation depends on specific requirements\n",
    "#         pass\n",
    "\n",
    "#     def generate_subtrees(self, doc, target_word, max_size=3):\n",
    "#         # Generate subtrees from the parse tree\n",
    "#         # This function will likely be the most complex to implement\n",
    "#         pass\n",
    "\n",
    "#     def expand_feature_space(self, subtrees):\n",
    "#         # Handle compound nouns and multi-unit lexical items\n",
    "#         pass\n",
    "\n",
    "#     def vectorize_subtrees(self, subtrees):\n",
    "#         # Convert subtrees to vector representation\n",
    "#         pass\n",
    "\n",
    "#     def compute_similarity(self, vector1, vector2):\n",
    "#         # Compute syntactic similarity using a graphlet-based approach\n",
    "#         # Example: Cosine similarity, Jaccard index, etc.\n",
    "#         pass\n",
    "\n",
    "#     def get_syntactic_similarity(self, sentence1, sentence2, target_word):\n",
    "#         doc1 = self.parse_sentence(sentence1)\n",
    "#         doc2 = self.parse_sentence(sentence2)\n",
    "\n",
    "#         simplified_tree1 = self.simplify_tree(doc1)\n",
    "#         simplified_tree2 = self.simplify_tree(doc2)\n",
    "\n",
    "#         subtrees1 = self.generate_subtrees(simplified_tree1, target_word)\n",
    "#         subtrees2 = self.generate_subtrees(simplified_tree2, target_word)\n",
    "\n",
    "#         expanded_subtrees1 = self.expand_feature_space(subtrees1)\n",
    "#         expanded_subtrees2 = self.expand_feature_space(subtrees2)\n",
    "\n",
    "#         vector1 = self.vectorize_subtrees(expanded_subtrees1)\n",
    "#         vector2 = self.vectorize_subtrees(expanded_subtrees2)\n",
    "\n",
    "#         return self.compute_similarity(vector1, vector2)\n",
    "\n",
    "# # Example usage\n",
    "# model = SyntacticSimilarityModel(language='en')\n",
    "# similarity = model.get_syntactic_similarity(\"He is a fast runner\", \"She is a slow runner\", \"runner\")\n",
    "# print(f\"Syntactic similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '田中さんはカレーを食べました。'\n",
    "s = '私は毎日青い自転車を京都大学まで漕ぎます。' # from the example of tolmachev\n",
    "s = '可愛い田中さんはカレーを食べました。' \n",
    "doc = nlp(s)\n",
    "doc = list(doc.sents)[0] # to get the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[可愛い, 田中さんは, カレーを, 食べました。]\n",
      "ADJ 可愛い\n",
      "NOUN さん\n",
      "NOUN カレー\n",
      "VERB 食べ\n",
      "[可愛い, さん, カレー, 食べ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ADJ', '田中', 'NOUN', 'は', 'NOUN', 'を', 'VERB', 'まし', 'た', '。']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bunsetsu extraction pos is the english name, tag is the japanese one\n",
    "bunsetsu_list = ginza.bunsetu_spans(doc)\n",
    "print(bunsetsu_list)\n",
    "# erasure\n",
    "# first must split?\n",
    "bunsetsu_head_list = ginza.bunsetu_head_tokens(doc)\n",
    "for t in bunsetsu_head_list:\n",
    "    # should change the tokens in the head with their Pos.\n",
    "    # here we have still distinction between noun and proper noun.\n",
    "    #span_pos = [t.pos_ for t in span]\n",
    "    # using t.pos_ instead of t.text or span.text\n",
    "\n",
    "    print(t.pos_, t.text)\n",
    "    #print(span.label_, span.text, span_pos)\n",
    "\n",
    "bunsetsu_func_list = ginza.bunsetu_head_tokens(doc)\n",
    "print(bunsetsu_func_list)\n",
    "\n",
    "\n",
    "[t.text if t not in bunsetsu_head_list else t.pos_ for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunsetsu_sub(token):\n",
    "    \"\"\"Performs substitution similar to Tolmachev et al.\"\"\"\n",
    "    bunsetsu_head_list = list(ginza.bunsetu_head_tokens(token.doc))\n",
    "    if token not in bunsetsu_head_list:\n",
    "        return token.lemma_\n",
    "    else: return token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[可愛い, 田中, さん, は, カレー, を, 食べ, まし, た, 。]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[は]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[2]\n",
    "ginza.is_bunsetu_head(token)\n",
    "list(ginza.rights(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "食べ"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = ginza.bunsetu_span(token)\n",
    "ginza.head(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree root node:  漕ぎ 漕ぎ+ます+。 漕ぎ\n",
      "node:  私+は <function traverse.<locals>.<lambda> at 0x7f6543ef9310> <function traverse.<locals>.<lambda> at 0x7f6543ef93a0>\n",
      "nsubj 私+は\n",
      "node:  毎日 <function traverse.<locals>.<lambda> at 0x7f6543ef9310> <function traverse.<locals>.<lambda> at 0x7f6543ef93a0>\n",
      "advmod 毎日\n",
      "node:  自転車+を <function traverse.<locals>.<lambda> at 0x7f6543ef9310> <function traverse.<locals>.<lambda> at 0x7f6543ef93a0>\n",
      "obj 自転車+を\n",
      "node:  京都大学+まで <function traverse.<locals>.<lambda> at 0x7f6543ef9310> <function traverse.<locals>.<lambda> at 0x7f6543ef93a0>\n",
      "obl 京都大学+まで\n"
     ]
    }
   ],
   "source": [
    "print('tree root node: ', doc.root, ginza.bunsetu(doc.root), ginza.phrase(doc.root))\n",
    "for rel, sb in ginza.sub_phrases(doc.root, ginza.bunsetu):\n",
    "    print('node: ', sb, ginza.bunsetu(sb), ginza.phrase(sb))\n",
    "    print(rel, sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"ja\" id=\"e92bc089117745d4ab5458094ca71da4-0\" class=\"displacy\" width=\"1800\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">私</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">は</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">毎日</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">青い</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">自転車</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">を</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">京都大学</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">まで</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">漕ぎ</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">ます。</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,2.0 1450.0,2.0 1450.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-1\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M205.0,441.5 L213.0,429.5 197.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-5\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M905.0,441.5 L913.0,429.5 897.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-6\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1435.0,264.5 1435.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-7\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1255.0,441.5 L1263.0,429.5 1247.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e92bc089117745d4ab5458094ca71da4-0-8\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e92bc089117745d4ab5458094ca71da4-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1605.0,441.5 L1613.0,429.5 1597.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Visualize the dependency parse\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
